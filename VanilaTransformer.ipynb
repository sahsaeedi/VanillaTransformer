{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7puriXnyrAuH",
        "outputId": "cf25e92d-35b0-4205-befa-155052f97310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-28 06:40:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-01-28 06:40:02 (17.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        " !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "3XEnqb5mttn0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUzabQZ0t7yG",
        "outputId": "6c9571ae-a2a7-4b61-a460-a0058bbac3f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIYiz6ZZuNd6",
        "outputId": "0fdc47ae-ed8d-4521-8b84-636cbd744df1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they thi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD-ymdFcuRut",
        "outputId": "3fc3f25c-6214-4073-c8dd-20e665a9daad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"Hello Amir\"))\n",
        "print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "#we can use other methods for encodoing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMIrk4Drum5O",
        "outputId": "3de615bc-ca1a-4f9e-8ede-748ae63e26d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 50, 50, 53, 1, 13, 51, 47, 56]\n",
            "Hello Amir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEHICu4zvvPR",
        "outputId": "703e64b0-5ac0-4900-d70f-5fc55aac5a9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "_jT4pwWTwvn_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnpQdJqaxLbU",
        "outputId": "a498137c-9202-485b-dcdc-4b3c8a3a9122"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D30P1ooaxiFz",
        "outputId": "cc323256-e6a6-4010-f9db-6525c7fc74c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  \n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('---------')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nns7kO6qycz6",
        "outputId": "c0935a12-60b0-4e01-f6a7-77cecd96c9ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "---------\n",
            "when input is [24] the target is 43\n",
            "when input is [24, 43] the target is 58\n",
            "when input is [24, 43, 58] the target is 5\n",
            "when input is [24, 43, 58, 5] the target is 57\n",
            "when input is [24, 43, 58, 5, 57] the target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
            "when input is [44] the target is 53\n",
            "when input is [44, 53] the target is 56\n",
            "when input is [44, 53, 56] the target is 1\n",
            "when input is [44, 53, 56, 1] the target is 58\n",
            "when input is [44, 53, 56, 1, 58] the target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52] the target is 58\n",
            "when input is [52, 58] the target is 1\n",
            "when input is [52, 58, 1] the target is 58\n",
            "when input is [52, 58, 1, 58] the target is 46\n",
            "when input is [52, 58, 1, 58, 46] the target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
            "when input is [25] the target is 17\n",
            "when input is [25, 17] the target is 27\n",
            "when input is [25, 17, 27] the target is 10\n",
            "when input is [25, 17, 27, 10] the target is 0\n",
            "when input is [25, 17, 27, 10, 0] the target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dypyAPG60k3K",
        "outputId": "d9868ea9-c81e-43e3-fba2-7f35efda3cb4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(-1) # targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      # print(i)\n",
        "      logits, loss = self(idx)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHkTP28Z12r7",
        "outputId": "8b3bb494-9c7d-4957-a0fb-46147c6d946b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "_u_-dmx72xxP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcx6ab9THwiu",
        "outputId": "4952409a-55dc-4912-82ca-3328cc57b215"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.382369041442871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvDPTw2PIHCN",
        "outputId": "96da83dd-12a5-438c-a428-6f88a2daed64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
            "RIfans picspeserer hee tha,\n",
            "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
            "ELIS me hurf lal y, ma dus pe athouo\n",
            "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
            "OLeneerithesinthengove fal amas trr\n",
            "TI ar I t, mes, n IUSt my w, fredeeyove\n",
            "THek' merer, dd\n",
            "We ntem lud engitheso; cer ize helorowaginte the?\n",
            "Thak orblyoruldvicee chot, p,\n",
            "Bealivolde Th li\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# #Hyperparameters\n",
        "# batch_size = 32\n",
        "# block_size = 8\n",
        "# max_iters = 3000\n",
        "# eval_iters = 300\n",
        "# learning_rate = 1e-2\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# torch.manual_seed(1337)\n",
        "# # wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "#   text = f.read()\n",
        "\n",
        "# chars = sorted(list(set(text)))\n",
        "# vocab_size = len(chars)\n",
        "\n",
        "\n",
        "# stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "# itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "# encode = lambda s: [stoi[c] for c in s]\n",
        "# decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# # print(encode(\"Hello Amir\"))\n",
        "# # print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "# #we can use other methods for encodoing\n",
        "# data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# n = int(0.9*len(data))\n",
        "# train_data = data[:n]\n",
        "# val_data = data[n:]\n",
        "\n",
        "# def get_batch(split):\n",
        "#   data = train_data if split == 'train' else val_data\n",
        "#   ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "#   x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "#   y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "#   x, y = x.to(device), y.to(device)\n",
        "#   return x, y\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def estimate_loss():\n",
        "#     out = {}\n",
        "#     model.eval()\n",
        "#     for split in ['train', 'val']:\n",
        "#         losses = torch.zeros(eval_iters)\n",
        "#         for k in range(eval_iters):\n",
        "#             X, Y = get_batch(split)\n",
        "#             logits, loss = model(X, Y)\n",
        "#             losses[k] = loss.item()\n",
        "#         out[split] = losses.mean()\n",
        "#     model.train()\n",
        "#     return out\n",
        "\n",
        "# class BigramLanguageModel(nn.Module):\n",
        "#   def __init__(self, vocab_size):\n",
        "#     super().__init__()\n",
        "#     self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "\n",
        "#   def forward(self, idx, targets=None):\n",
        "#     logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "#     if targets is None:\n",
        "#       loss = None\n",
        "#     else:\n",
        "#       B, T, C = logits.shape\n",
        "#       logits = logits.view(B*T, C)\n",
        "#       targets = targets.view(-1) # targets.view(B*T)\n",
        "#       loss = F.cross_entropy(logits, targets)\n",
        "#     return logits, loss\n",
        "\n",
        "#   def generate(self, idx, max_new_tokens):\n",
        "#     for _ in range(max_new_tokens):\n",
        "#       # print(i)\n",
        "#       logits, loss = self(idx)\n",
        "#       logits = logits[:,-1,:]\n",
        "#       probs = F.softmax(logits, dim = -1)\n",
        "#       idx_next = torch.multinomial(probs, num_samples=1)\n",
        "#       idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "#     return idx\n",
        "\n",
        "# model = BigramLanguageModel(vocab_size)\n",
        "# m = model.to(device)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "# for iter in range(max_iters):\n",
        "\n",
        "#     if iter % eval_iters == 0:\n",
        "#         losses = estimate_loss()\n",
        "#         print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "#     xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "#     logits, loss = model(xb, yb)\n",
        "#     optimizer.zero_grad(set_to_none=True)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "yLIKReU2IgCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81abcf4-4c8d-4a12-f1c4-c17f0d24494c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7293, val loss 4.7267\n",
            "step 300: train loss 2.8077, val loss 2.8241\n",
            "step 600: train loss 2.5449, val loss 2.5570\n",
            "step 900: train loss 2.5049, val loss 2.5169\n",
            "step 1200: train loss 2.4773, val loss 2.5016\n",
            "step 1500: train loss 2.4757, val loss 2.4977\n",
            "step 1800: train loss 2.4725, val loss 2.4897\n",
            "step 2100: train loss 2.4682, val loss 2.4961\n",
            "step 2400: train loss 2.4711, val loss 2.4796\n",
            "step 2700: train loss 2.4706, val loss 2.4928\n",
            "\n",
            "Gofithirdo sichame,\n",
            "Whe,\n",
            "\n",
            "ANOMo SIOUKIONCERI os'IUSes ste?\n",
            "oor mealintimatede ser movis non. h ge R a s, wind ngulffove ou by w nariravak ththous d wans thashe belled anthin, sk theander y I,\n",
            "S anten\n",
            "Gor t th. lo CKE:\n",
            "INawhin wed fur prerdy higse lom;\n",
            "WIUCont\n",
            "piekimbentry bagure med anon:\n",
            "O, owe y y'ld be,\n",
            "I pond,\n",
            "netcosouFliule.\n",
            "INGl,\n",
            "ICEToungrrdonil k Bur s thupow ced w.\n",
            "SEHe, ase ic parl gum hangr t w fune hen seringou INVat? US:COUpoof,\n",
            "Whred t myeaverentcooshe, whiks d.\n",
            "D pal'lsp\n",
            "Thad! sat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4, 8, 2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2joKhj2OkNC",
        "outputId": "9e5d1689-6b7a-470b-8f8e-583401c40160"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1]\n",
        "    xbow[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "ZV4qBYaKMZFW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim = True)\n",
        "xbow2 = wei @ x\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqXRxH2FTd_q",
        "outputId": "7d032f80-0918-417a-edbe-7ba054aacde9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2trifZ9MTfMH",
        "outputId": "35bf6c54-046c-43c3-b173-2cb158c56d2f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query vector : What matter looking for?\n",
        "# Key vecotr : What do I contain?"
      ],
      "metadata": {
        "id": "HtYucgt2Yles"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4, 8, 32\n",
        "x = torch.randn(B,T,C)\n",
        "# x.shape\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl12SdowXdZc",
        "outputId": "2fe63f97-3583-473e-86e3-3f258b1e762f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size ** -0.5"
      ],
      "metadata": {
        "id": "GY1mn0eUdKUs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIRQ6GRFdZQv",
        "outputId": "d79abaf6-59df-44ea-9e15-1de28f3814c3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxCo9RbKdZMu",
        "outputId": "96771ef1-d7b2-4371-a0c8-75e1e002f014"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9006)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsyD3KfIdZDB",
        "outputId": "51b98c5b-95ef-4eca-d364-9b13f16c0737"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1277)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim = True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('--')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7LyNVSpPEPp",
        "outputId": "e7a2697e-5538-47b8-842f-d4f5b495e3fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# print(encode(\"Hello Amir\"))\n",
        "# print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "#we can use other methods for encodoing\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(-1) # targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # print(i)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUylE0I8PGvh",
        "outputId": "a2b203ca-754c-4863-8011-1311bc75298b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2000, val loss 4.2047\n",
            "step 500: train loss 2.6911, val loss 2.7087\n",
            "step 1000: train loss 2.5196, val loss 2.5303\n",
            "step 1500: train loss 2.4775, val loss 2.4829\n",
            "step 2000: train loss 2.4408, val loss 2.4523\n",
            "step 2500: train loss 2.4272, val loss 2.4435\n",
            "step 3000: train loss 2.4130, val loss 2.4327\n",
            "step 3500: train loss 2.3956, val loss 2.4212\n",
            "step 4000: train loss 2.4041, val loss 2.3992\n",
            "step 4500: train loss 2.3980, val loss 2.4084\n",
            "\n",
            "K:\n",
            "NGey\n",
            "\n",
            "Letnrad wineam:\n",
            "Kicou hitipteavimancraby whet muthe hus darge.\n",
            "\n",
            "Wind!\n",
            "IRD: Ind, tind spoof om and f.\n",
            "Sy stllalevere here me honouen fot in,\n",
            "So and, vist orby?\n",
            "Thar hous mat deest she rd?\n",
            "\n",
            "Wowin wof t, ath th ay miligiryouchth-orto mou tenges, ald pors banebe y prothetack aklel I veriplansnidierd avit for,\n",
            "KI thit ndist allll perd the:\n",
            "Acu Empoouthant, I to\n",
            "Ten mar.\n",
            "\n",
            "S:\n",
            "Bugh the I hy nd meis moh h!\n",
            "\n",
            "\n",
            "AThamen es ty I has.\n",
            "\n",
            "MI ithe thensterat blo gaar,\n",
            "A d muts ed ronur wiend tl-ou,\n",
            "Therim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# print(encode(\"Hello Amir\"))\n",
        "# print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "#we can use other methods for encodoing\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attenstion in parallel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "       super().__init__()\n",
        "       self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # self.sa_head = Head(n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(-1) # targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # print(i)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AaC4KGNgqmw",
        "outputId": "a3b17efc-f366-4e04-998a-4a7fd3466415"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2227, val loss 4.2226\n",
            "step 500: train loss 2.6592, val loss 2.6733\n",
            "step 1000: train loss 2.4980, val loss 2.5064\n",
            "step 1500: train loss 2.4291, val loss 2.4349\n",
            "step 2000: train loss 2.3716, val loss 2.3844\n",
            "step 2500: train loss 2.3417, val loss 2.3561\n",
            "step 3000: train loss 2.3149, val loss 2.3347\n",
            "step 3500: train loss 2.2918, val loss 2.3171\n",
            "step 4000: train loss 2.2895, val loss 2.2868\n",
            "step 4500: train loss 2.2748, val loss 2.2858\n",
            "\n",
            "KOMNG\n",
            "yous'd rad wineam:\n",
            "\n",
            "Soou hitis eavim Is\n",
            "Aby? lligm the hus darge.\n",
            "\n",
            "Win bregun cout tind spooou meand for.\n",
            "\n",
            "QANCOMNE Se hate my houtoen fot in,\n",
            "Shour so the ors\n",
            "And yoourturm, fakest shibre?\n",
            "\n",
            "Wowit wee the the hany milighereeequith to mou thou s, st sp steranebe you on htak be lellilver?\n",
            "Salsnidierd bu thy are het, ton nithat.\n",
            "\n",
            "FANG ENRL:\n",
            "Acu Empooull of What hienaugr.\n",
            "\n",
            "BOUCER:\n",
            "Whe I dyen, mey froh hott\n",
            "A mambite pry'll low's wit:\n",
            "Youn,\n",
            "RTerat blougear,\n",
            "A dim wiselar,\n",
            "Wre tit yoou llaverdim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# print(encode(\"Hello Amir\"))\n",
        "# print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "#we can use other methods for encodoing\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attenstion in parallel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "       super().__init__()\n",
        "       self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # self.sa_head = Head(n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffwd(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(-1) # targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # print(i)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45YgfFIbiZ0Q",
        "outputId": "04baf401-c2a9-4d4d-fd6a-5f5bb2bc419e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1996, val loss 4.1995\n",
            "step 500: train loss 2.5993, val loss 2.6077\n",
            "step 1000: train loss 2.4629, val loss 2.4651\n",
            "step 1500: train loss 2.3974, val loss 2.3951\n",
            "step 2000: train loss 2.3297, val loss 2.3470\n",
            "step 2500: train loss 2.3018, val loss 2.3221\n",
            "step 3000: train loss 2.2828, val loss 2.2936\n",
            "step 3500: train loss 2.2495, val loss 2.2721\n",
            "step 4000: train loss 2.2435, val loss 2.2468\n",
            "step 4500: train loss 2.2286, val loss 2.2411\n",
            "\n",
            "Yuwol herim bue!\n",
            "\n",
            "KO:\n",
            "Giou hals arern.\n",
            "The: desee?\n",
            "Ill be wote, pon uedecon uf hill I vite woths\n",
            "the is tild id thoth ant the not; fet leay it yous to kifgis I towtich mee; hace ders, ink tharchou you ther tell, vadaren, pailest.\n",
            "\n",
            "Ge this cut uf het weme to.\n",
            "\n",
            "Bloyect lit licead arn Anod mefams on it ther st poly this.\n",
            "\n",
            "Sord tincavels,\n",
            "Anwerse thand slinetrs;\n",
            "Whoms pabund my hist ther futedinve\n",
            "DOurssot eperast he not?\n",
            "\n",
            "To Enisiver fond your the, to it to pook,\n",
            "Pry fut whallis piysing sarceestis \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# print(encode(\"Hello Amir\"))\n",
        "# print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "#we can use other methods for encodoing\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attenstion in parallel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "       super().__init__()\n",
        "       self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "       self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block : communication followed by computation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "       super().__init__()\n",
        "       head_size = n_embd // n_head\n",
        "       self.sa = MultiHeadAttention(n_head, head_size)\n",
        "       self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # self.sa_head = Head(n_embd)\n",
        "    # self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "    # self.ffwd = FeedForward(n_embd)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    # x = self.sa_heads(x)\n",
        "    # x = self.ffwd(x)\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(-1) # targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # print(i)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eNj60f6j4Ru",
        "outputId": "5195a45e-5288-47b4-f384-c72cfefb8ad4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6255, val loss 4.6233\n",
            "step 500: train loss 2.3884, val loss 2.3850\n",
            "step 1000: train loss 2.2706, val loss 2.2686\n",
            "step 1500: train loss 2.1877, val loss 2.2104\n",
            "step 2000: train loss 2.1467, val loss 2.1828\n",
            "step 2500: train loss 2.1082, val loss 2.1542\n",
            "step 3000: train loss 2.0710, val loss 2.1439\n",
            "step 3500: train loss 2.0620, val loss 2.1190\n",
            "step 4000: train loss 2.0259, val loss 2.1101\n",
            "step 4500: train loss 2.0045, val loss 2.1037\n",
            "\n",
            "she doward.\n",
            "Vollamy as prow bay my for som duvist clow. This satule a dutlette,\n",
            "The dowaid your sit,\n",
            "But Gave kis have mellan for and youne bele I senty but will you.\n",
            "\n",
            "DUKES Seor of this frean'd to purdin? Unot in the priss to is harce, beadrespery, as thou dome, whet Romeserry, solde's you camme sowered.\n",
            "Huss Jor our to the loour tuke you, fromas? Burse ste's dime is sey up your staysents. UII Must and ceace.\n",
            "\n",
            "MENCELIUS:\n",
            "Aried are ay stan\n",
            "The that epdelow: fear hear you\n",
            "The for our wild henowd \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 1e-3\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# print(encode(\"Hello Amir\"))\n",
        "# print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "#we can use other methods for encodoing\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attenstion in parallel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "       super().__init__()\n",
        "       self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "       self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block : communication followed by computation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "       super().__init__()\n",
        "       head_size = n_embd // n_head\n",
        "       self.sa = MultiHeadAttention(n_head, head_size)\n",
        "       self.ffwd = FeedForward(n_embd)\n",
        "       self.ln1 = nn.LayerNorm(n_embd)\n",
        "       self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # self.sa_head = Head(n_embd)\n",
        "    # self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "    # self.ffwd = FeedForward(n_embd)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        nn.LayerNorm(n_embd),\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    # x = self.sa_heads(x)\n",
        "    # x = self.ffwd(x)\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(-1) # targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # print(i)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiHNmWpblbZ7",
        "outputId": "b32ea569-4530-48b6-e862-a21e2e743338"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5361, val loss 4.5363\n",
            "step 500: train loss 2.4011, val loss 2.4016\n",
            "step 1000: train loss 2.2738, val loss 2.2777\n",
            "step 1500: train loss 2.1930, val loss 2.2101\n",
            "step 2000: train loss 2.1494, val loss 2.1885\n",
            "step 2500: train loss 2.1094, val loss 2.1506\n",
            "step 3000: train loss 2.0746, val loss 2.1457\n",
            "step 3500: train loss 2.0668, val loss 2.1175\n",
            "step 4000: train loss 2.0336, val loss 2.1080\n",
            "step 4500: train loss 2.0071, val loss 2.0960\n",
            "\n",
            "shald, and men this at land bay. Unge you? healf, well. This satule medure\n",
            "the,\n",
            "The dowaid you you; hichear; life have mells gectarten you knole I tentyen? Gronand eof I' wim.\n",
            "\n",
            "GFONCUS:\n",
            "Come ark to purdine the? in the prisa to in horcomeder.\n",
            "\n",
            "ESANGFS:\n",
            "Be of mock, whet of and\n",
            "Sther of 't you camme syelmed.\n",
            "Hus fance do to the look thuss fill for as? buth.\n",
            "\n",
            "ISIVOBESS OLING ELYANGUS:\n",
            "Why Enown bus but sencleed to for his love riestare, your with hem of prentars may heir gram to jought that hornow\n",
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 3e-4\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# print(encode(\"Hello Amir\"))\n",
        "# print(decode(encode(\"Hello Amir\")))\n",
        "\n",
        "#we can use other methods for encodoing\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "       self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attenstion in parallel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "       super().__init__()\n",
        "       self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "       self.proj = nn.Linear(n_embd, n_embd)\n",
        "       self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block : communication followed by computation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "       super().__init__()\n",
        "       head_size = n_embd // n_head\n",
        "       self.sa = MultiHeadAttention(n_head, head_size)\n",
        "       self.ffwd = FeedForward(n_embd)\n",
        "       self.ln1 = nn.LayerNorm(n_embd)\n",
        "       self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # self.sa_head = Head(n_embd)\n",
        "    # self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "    # self.ffwd = FeedForward(n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    # self.blocks = nn.Sequential(\n",
        "    #     Block(n_embd, n_head=4),\n",
        "    #     Block(n_embd, n_head=4),\n",
        "    #     Block(n_embd, n_head=4),\n",
        "    #     nn.LayerNorm(n_embd),\n",
        "    # )\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    # x = self.sa_heads(x)\n",
        "    # x = self.ffwd(x)\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(-1) # targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      # print(i)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "iosHjODtoXiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NvlF58Iqys9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}